{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vVmOpZj146dz","colab_type":"code","outputId":"824f51f5-a24d-435b-da42-57193b832b22","executionInfo":{"status":"ok","timestamp":1574910711841,"user_tz":-480,"elapsed":39135,"user":{"displayName":"lei sheng","photoUrl":"","userId":"13894588778487313107"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q7pXQ1vDab5l","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LtOVqlGTm47m","colab_type":"code","outputId":"a8d8d837-b870-4664-ccbb-385a58ab5e92","executionInfo":{"status":"ok","timestamp":1574840773700,"user_tz":-480,"elapsed":1062064,"user":{"displayName":"lei sheng","photoUrl":"","userId":"13894588778487313107"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["#!wget http://data.csail.mit.edu/places/places205/testSetPlaces205_resize.tar.gz\n","#!tar -xzf testSetPlaces205_resize.tar.gz"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-11-27 07:28:34--  http://data.csail.mit.edu/places/places205/testSetPlaces205_resize.tar.gz\n","Resolving data.csail.mit.edu (data.csail.mit.edu)... 128.52.129.40\n","Connecting to data.csail.mit.edu (data.csail.mit.edu)|128.52.129.40|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2341250899 (2.2G) [application/octet-stream]\n","Saving to: ‘testSetPlaces205_resize.tar.gz’\n","\n","testSetPlaces205_re 100%[===================>]   2.18G  16.7MB/s    in 2m 58s  \n","\n","2019-11-27 07:31:32 (12.6 MB/s) - ‘testSetPlaces205_resize.tar.gz’ saved [2341250899/2341250899]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J5-i3QvwyuT8","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqEyhV6jyuUA","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","import cv2\n","from PIL import Image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YP82wCUyuUC","colab_type":"code","colab":{}},"source":["import numpy as np\n","import datetime\n","import os, sys\n","import torchvision.models as models\n","from torchvision import datasets, transforms"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PEYsXLHnIfw","colab_type":"code","colab":{}},"source":["from skimage.color import lab2rgb, rgb2lab, rgb2gray\n","from skimage import io"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJWqFB7XgPbE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUv73eZ6dC_d","colab_type":"code","colab":{}},"source":["#彩色图像转为灰度图像\n","def rgb2gray(img_path):#传入的是路径\n","    img = Image.open(img_path)\n","    return img.convert('L')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZiNARcItUdy","colab_type":"code","colab":{}},"source":["#RGB转LAB\n","def rgb2lab(pic_file):#传入的是路径\n","  img_bgr = cv2.imread(pic_file, cv2.IMREAD_COLOR) #OpenCV读取颜色顺序：BRG \n","  img_lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n","  return img_lab\n","#opencv 的Lab数据对齐做了量化，使其处于0-255范围L=L*2.55,a=a+128,b=b+128"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_xi0Mlm7xZy","colab_type":"code","colab":{}},"source":["DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VO6ielkROcHh","colab_type":"code","colab":{}},"source":["target_transform =transforms.Compose([transforms.Resize((64,64)),\n","                                transforms.ToTensor(),\n","                                #transforms.Lambda(lambda x: x.repeat(3,1,1)),\n","                                transforms.Normalize(mean=(0.5, 0.5, 0.5),\n","                                std=(0.5, 0.5, 0.5))\n","                               ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgjyHKEpyuUb","colab_type":"code","colab":{}},"source":["transform = transforms.Compose([transforms.Resize((64,64)),\n","                                transforms.ToTensor(),\n","                                #transforms.Lambda(lambda x: x.repeat(3,1,1)),\n","                                transforms.Normalize(mean=(0.5, 0.5, 0.5),\n","                                std=(0.5, 0.5, 0.5))\n","                               ]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZZodJwqsxkN","colab_type":"text"},"source":["#上层采样，用迁移学习ResNet18融合"]},{"cell_type":"code","metadata":{"id":"7alu1I0MmN4P","colab_type":"code","colab":{}},"source":["class ColorizationNet(nn.Module):\n","  def __init__(self,input_size=128):\n","    super(ColorizationNet,self).__init__()\n","\n","    #First，\n","    resnet=models.resnet18()#(num_classes=365)\n","    #改变第一个卷积层接受灰度图像\n","    resnet.conv1.weight=nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1))#在第2个维度上增加1\n","    #扩展特征从 Resnet-gray\n","    self.midlevel_resnet=nn.Squential(*list(resnet.children())[0:6])#取Resnet-18的前6层\n","    ## 没有参数，速度更快，采取给定策略上采样\n","    #torch.nn.Upsample(size=输出矩阵, scale_factor=None(图像宽/高/深度的倍数), mode='nearest', align_corners=None)\n","    self.upsample=nn.Sequential(\n","        nn.ReLU(),\n","        nn.Conv2d(128,128,3,1,padding=1),\n","        nn.BatchNorm1d(128),\n","        nn.ReLU(),\n","        nn.Upsample(scale_factor=2),\n","        \n","        nn.ReLU(),\n","        nn.Conv2d(128,64,3,1,padding=1),\n","        nn.BatchNorm1d(128),\n","        nn.ReLU(),\n","        nn.Upsample(scale_factor=2),\n","\n","        nn.ReLU(),\n","        nn.Conv2d(64,64,3,1,padding=1),\n","        nn.BatchNorm1d(64),\n","        nn.ReLU(),\n","        nn.Upsample(scale_factor=2),\n","\n","        nn.ReLU(),\n","        nn.Conv2d(64,32,3,1,padding=1),\n","        nn.BatchNorm1d(32),\n","        nn.ReLU(),\n","\n","        nn.Conv2d(32,2,3,1,padding=1),\n","        nn.BatchNorm2d(2),\n","        nn.Upsample(scale_factor=2),\n","        nn.Tanh(),\n","      )\n","    def forward(self,input):\n","      #把输入的灰度图像到特征\n","      midlevel_features=self.midlevel_resnet(input)\n","      #通过上采样到上色(反卷积的过程)\n","      output=self.upsample(midlevel_features)\n","      return output\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NqZ3DUcxyuUO","colab_type":"code","colab":{}},"source":["class Discriminator(nn.Module):\n","    \"\"\"\n","        Convolutional Discriminator for \n","    \"\"\"\n","    def __init__(self, in_channel=3, num_classes=1):\n","        super(Discriminator, self).__init__()\n","        self.conv = nn.Sequential(\n","            #64，64\n","            nn.Conv2d(in_channel, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(0.2),\n","            #->\n","            nn.Conv2d(32,64,3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","            # 64，64-》33，33\n","            nn.Conv2d(64, 64,3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","            nn.MaxPool2d(2, 2,padding=(1,1)),#kernel_size, stride=None, padding=0,\n","            # 33，33-》16，16\n","            nn.Conv2d(64,128,3, stride=2, padding=1, bias=False),          \n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","            #nn.AvgPool2d(4),\n","            nn.AdaptiveAvgPool2d(16),\n","        )\n","        self.fc = nn.Sequential(\n","            # reshape input, 128 -> 1\n","            nn.Linear(128, 1),\n","            nn.Sigmoid(),\n","        )\n","    \n","    def forward(self, x, y=None):\n","        y_ = self.conv(x)\n","        y_ = y_.view(y_.size(0), -1)\n","        y_ = self.fc(y_)\n","        return y_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZNUAotJpuOb6","colab_type":"text"},"source":["创建模型"]},{"cell_type":"code","metadata":{"id":"VAOpQVabt_OT","colab_type":"code","colab":{}},"source":["G=ColorizationNet()\n","D=Discriminator()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3sSVWskwuY_R","colab_type":"text"},"source":["训练"]},{"cell_type":"code","metadata":{"id":"N2v_2GgDubVI","colab_type":"code","colab":{}},"source":["criterion=nn.KLDivloss(reduce=False)#输入必须是概率的log形式\n","optimizer=torch.optim.Adam(model.parameters(),lr=1e-2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2LxhvZHcf8Nk","colab":{}},"source":["#AB[-127,128]\n","class ColorImage(Dataset):\n","    def __init__(self, data_path,transform=None,target_transform=None):\n","        '''\n","        Args:\n","            data_path (str): path to dataset\n","        '''\n","        images=os.listdir(data_path)\n","        self.images=[os.path.join(data_path,image) for image in images if image.endswith('.jpg')]\n","        self.transform=transform\n","        self.target_transform=target_transform\n","        \n","    def __getitem__(self,idx):\n","        image_path=self.images[idx]\n","        #img=Image.open(image_path)\n","        original_image=Image.open(image_path)\n","        pil_image=rgb2lab(image_path)[:,:,0]\n","        pil_image_Y=rgb2lab(image_path)[:,:,1:]#运用cv2得到的ab通道就是\n","        #pil_image_Y=pil_image_Y/128#ab色谱的取值范围[-128,127]->[-1,1]与神经网络进行匹配\n","        pil_image_L1=Image.fromarray(pil_image_L)#把矩阵转为image\n","        pil_image_Y1=Image.fromarray(pil_image_Y)\n","        if self.transform is not None:\n","          data_L=self.transform(pil_image_L1)\n","          original_image=self.transform(original_image)\n","          \n","        if self.target_transform is not None:\n","          data_Y=self.target_transform(pil_image_Y1)\n","        else:\n","          image_array1 = np.asarray(pil_image_L1)#转为数组\n","          data_L = torch.from_numpy(image_array1.transpose((2,0,1)).float())#转为tensor\n","          image_array2 = np.asarray(pil_image_Y1)\n","          data_Y = torch.from_numpy(image_array2.unsqueeze(0).float())\n","          image_array3= np.asarray(original_image)#转为数组\n","          original_image = torch.from_numpy(original_image.transpose((2,0,1)).float())#转为tensor    \n","        return data_L,data_Y,original_image#返回一个单通道的,一个ab通道的彩色图像,原来的RGB图像\n","\n","    def __len__(self):\n","        return len(self.images)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-66ZJ4737u6T","colab_type":"text"},"source":["训练之前，定义函数跟踪训练损失并将图像转为RGB"]},{"cell_type":"code","metadata":{"id":"yB_J62KwrkPX","colab_type":"code","colab":{}},"source":["class AverageMeter(object):\n","  '''A handy class from the PyTorch ImageNet tutorial''' \n","  def __init__(self):\n","    self.reset()\n","  def reset(self):\n","    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n","  def update(self, val, n=1):\n","    self.val = val\n","    self.sum += val * n\n","    self.count += n\n","    self.avg = self.sum / self.count\n","\n","def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n","  '''Show/save rgb image from grayscale and ab channels\n","     Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n","  plt.clf() # clear matplotlib \n","  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n","  color_image = color_image.transpose((1, 2, 0))  # 缩放for matplotlib\n","  color_image[:, :, 0:1] = color_image[:, :, 0:1] #* 100\n","  color_image[:, :, 1:3] = color_image[:, :, 1:3] #* 255 - 128   \n","  color_image = lab2rgb(color_image.astype(np.float64))\n","  grayscale_input = grayscale_input.squeeze().numpy()\n","  if save_path is not None and save_name is not None: \n","    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n","    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))\n","  return color_g_image"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Te92V80yz0XJ","colab_type":"text"},"source":["数据"]},{"cell_type":"code","metadata":{"id":"SDRxvRd3iy4z","colab_type":"code","colab":{}},"source":["dataset1= ColorImage(data_path='./dataset', transform=transform,target_transform=target_transform)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpCKzqMHzdVh","colab_type":"code","colab":{}},"source":["batch_size =64\n","data_loader= DataLoader(dataset=dataset1, batch_size=batch_size, shuffle=True,drop_last=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4r6TD2AB6sF","colab_type":"code","colab":{}},"source":["max_epoch = 100\n","step = 0\n","n_critic = 1 # for training more k steps about Discriminator\n","n_noise = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DURv7kB39nGL","colab":{}},"source":["max_epoch = 100\n","step = 0\n","n_critic = 1 # for training more k steps about Discriminator\n","n_noise = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"WV7PvZ-JB6sR","colab_type":"code","outputId":"044791f9-9bf1-40d2-e941-5d4c43811321","colab":{}},"source":["for epoch in range(max_epoch):\n","    for idx,(gray_images,ab_images,oroginal_img) in enumerate(data_loader):\n","        color_rgb_g_image=to_rgb(gray_images,ab_images)\n","        # Training Discriminator\n","        x =oroginal_img.to(DEVICE)\n","        x_outputs = D(x)\n","        D_x_loss = criterion(x_outputs, D_labels)\n","\n","        #z = torch.randn(batch_size, n_noise).to(DEVICE)\n","        z_outputs = D(G(color_rgb_g_image))\n","        D_z_loss = criterion(z_outputs, D_fakes)\n","        D_loss = D_x_loss + D_z_loss\n","        \n","        D.zero_grad()\n","        D_loss.backward()\n","        D_opt.step()\n","\n","        if step % n_critic == 0:\n","            # Training Generator\n","            z=color_rgb_g_image\n","            z_outputs = D(G(z))\n","            G_loss = criterion(z_outputs, D_labels)\n","\n","            D.zero_grad()\n","            G.zero_grad()\n","            G_loss.backward()\n","            G_opt.step()\n","        \n","        if step % 500 == 0:\n","            dt = datetime.datetime.now().strftime('%H:%M:%S')\n","            print('Epoch: {}/{}, Step: {}, D Loss: {:.4f}, G Loss: {:.4f}, Time:{}'.format(epoch, max_epoch, step, D_loss.item(), G_loss.item(), dt))\n","            G.eval()\n","            \n","            G.train()\n","        step += 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 0/100, Step: 0, D Loss: 1.3999, G Loss: 0.6180, Time:16:46:45\n","Epoch: 3/100, Step: 500, D Loss: 1.1598, G Loss: 1.0826, Time:16:47:35\n","Epoch: 7/100, Step: 1000, D Loss: 0.9844, G Loss: 0.9956, Time:16:48:25\n","Epoch: 11/100, Step: 1500, D Loss: 0.9111, G Loss: 1.1050, Time:16:49:16\n","Epoch: 15/100, Step: 2000, D Loss: 0.8433, G Loss: 1.1394, Time:16:50:06\n","Epoch: 19/100, Step: 2500, D Loss: 1.0972, G Loss: 1.2265, Time:16:50:56\n","Epoch: 23/100, Step: 3000, D Loss: 0.9124, G Loss: 1.1806, Time:16:51:47\n","Epoch: 27/100, Step: 3500, D Loss: 0.7294, G Loss: 1.5435, Time:16:52:38\n","Epoch: 31/100, Step: 4000, D Loss: 0.8479, G Loss: 1.4291, Time:16:53:29\n","Epoch: 35/100, Step: 4500, D Loss: 0.7055, G Loss: 1.6143, Time:16:54:21\n","Epoch: 39/100, Step: 5000, D Loss: 0.5880, G Loss: 1.8291, Time:16:55:12\n","Epoch: 43/100, Step: 5500, D Loss: 0.7331, G Loss: 2.0989, Time:16:56:03\n","Epoch: 47/100, Step: 6000, D Loss: 1.1191, G Loss: 0.4572, Time:16:56:54\n","Epoch: 51/100, Step: 6500, D Loss: 0.5204, G Loss: 2.0754, Time:16:57:45\n","Epoch: 55/100, Step: 7000, D Loss: 0.7635, G Loss: 1.3319, Time:16:58:38\n","Epoch: 59/100, Step: 7500, D Loss: 0.7345, G Loss: 1.4658, Time:16:59:36\n","Epoch: 63/100, Step: 8000, D Loss: 0.5926, G Loss: 2.2090, Time:17:00:32\n","Epoch: 67/100, Step: 8500, D Loss: 0.4818, G Loss: 2.0671, Time:17:01:30\n","Epoch: 71/100, Step: 9000, D Loss: 0.5194, G Loss: 2.1079, Time:17:02:28\n","Epoch: 75/100, Step: 9500, D Loss: 0.4987, G Loss: 1.7474, Time:17:03:25\n","Epoch: 79/100, Step: 10000, D Loss: 0.4036, G Loss: 2.1554, Time:17:04:23\n","Epoch: 83/100, Step: 10500, D Loss: 1.2285, G Loss: 3.0286, Time:17:05:22\n","Epoch: 87/100, Step: 11000, D Loss: 0.3730, G Loss: 2.3211, Time:17:06:19\n","Epoch: 91/100, Step: 11500, D Loss: 0.3307, G Loss: 2.4072, Time:17:07:14\n","Epoch: 95/100, Step: 12000, D Loss: 0.4987, G Loss: 3.4012, Time:17:08:12\n","Epoch: 99/100, Step: 12500, D Loss: 0.4947, G Loss: 3.2567, Time:17:09:09\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ojON8P4cEBJE","colab_type":"code","outputId":"23c96b57-4f58-4e96-fe8a-3e4efbff0cd5","executionInfo":{"status":"error","timestamp":1574771442376,"user_tz":-480,"elapsed":2268,"user":{"displayName":"lei sheng","photoUrl":"","userId":"13894588778487313107"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["def validate(val_loader, model=G, criterion, save_images, epoch):\n","  model.eval()\n","\n","  # Prepare value counters and timers\n","  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n","\n","  end = time.time()\n","  already_saved_images = False\n","  for i, (input_gray, input_ab, target) in enumerate(val_loader):\n","    data_time.update(time.time() - end)\n","\n","    # Use GPU\n","    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n","\n","    # Run model and record loss\n","    output_ab = model(input_gray) # throw away class predictions\n","    loss = criterion(output_ab, input_ab)\n","    losses.update(loss.item(), input_gray.size(0))\n","\n","    # Save images to file\n","    if save_images and not already_saved_images:\n","      already_saved_images = True\n","      for j in range(min(len(output_ab), 10)): # save at most 5 images\n","        save_path = {'grayscale': 'outputs/gray/', 'colorized': 'outputs/color/'}\n","        save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n","        to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n","        # Record time to do forward passes and save images\n","    batch_time.update(time.time() - end)\n","    end = time.time()\n","\n","    # Print model accuracy -- in the code below, val refers to both value and validation\n","    if i % 25 == 0:\n","      print('Validate: [{0}/{1}]\\t'\n","            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n","             i, len(val_loader), batch_time=batch_time, loss=losses))\n","\n","  print('Finished validation.')\n","  return losses.avg"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0425dacf1ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlena\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'skimage.data' has no attribute 'lena'"]}]},{"cell_type":"markdown","metadata":{"id":"dWByLjUBtnVL","colab_type":"text"},"source":["训练"]},{"cell_type":"code","metadata":{"id":"YvEYfvokts0y","colab_type":"code","colab":{}},"source":["def train(train_loader, model, criterion, optimizer, epoch):\n","  print('Starting training epoch {}'.format(epoch))\n","  G.train()\n","  \n","  # Prepare value counters and timers\n","  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n","\n","  #end = time.time()\n","  for i, (input_gray, input_ab, target) in enumerate(train_loader):\n","    \n","    # Use GPU if available\n","    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n","\n","    # Record time to load data (above)\n","    data_time.update(time.time() - end)\n","\n","    # Run forward pass\n","    output_ab = model(input_gray) \n","    loss = criterion(output_ab, input_ab) \n","    losses.update(loss.item(), input_gray.size(0))\n","\n","    # Compute gradient and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Record time to do forward and backward passes\n","    batch_time.update(time.time() - end)\n","    end = time.time()\n","\n","    # Print model accuracy -- in the code below, val refers to value, not validation\n","    if i % 25 == 0:\n","      print('Epoch: [{0}][{1}/{2}]\\t'\n","            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n","              epoch, i, len(train_loader), batch_time=batch_time,\n","             data_time=data_time, loss=losses)) \n","\n","  print('Finished training epoch {}'.format(epoch))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqZxYKYmt0Dn","colab_type":"code","outputId":"a8b45145-eda7-4d91-fca3-16a2e2f9c087","executionInfo":{"status":"error","timestamp":1574838992675,"user_tz":-480,"elapsed":2800,"user":{"displayName":"lei sheng","photoUrl":"","userId":"13894588778487313107"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["os.makedirs('outputs/color', exist_ok=True)\n","os.makedirs('outputs/gray', exist_ok=True)\n","os.makedirs('checkpoints', exist_ok=True)\n","save_images = True\n","best_losses = 1e10\n","epochs = 100"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d612e328f31c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/color'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/gray'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msave_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Q1Y3c0XTt77C","colab_type":"code","colab":{}},"source":["# Train model\n","for epoch in range(epochs):\n","  # Train for one epoch, then validate\n","  train(train_loader, model, criterion, optimizer, epoch)\n","  with torch.no_grad():\n","    losses = validate(val_loader, model, criterion, save_images, epoch)\n","  # Save checkpoint and replace old best model if current model is better\n","  if losses < best_losses:\n","    best_losses = losses\n","    torch.save(model.state_dict(), 'checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3J3OAIjOuIOq","colab_type":"code","colab":{}},"source":["# Show images \n","import matplotlib.image as mpimg\n","image_pairs = [('outputs/color/img-2-epoch-0.jpg', 'outputs/gray/img-2-epoch-0.jpg'),\n","               ('outputs/color/img-7-epoch-0.jpg', 'outputs/gray/img-7-epoch-0.jpg')]\n","for c, g in image_pairs:\n","  color = mpimg.imread(c)\n","  gray  = mpimg.imread(g)\n","  f, axarr = plt.subplots(1, 2)\n","  f.set_size_inches(15, 15)\n","  axarr[0].imshow(gray, cmap='gray')\n","  axarr[1].imshow(color)\n","  axarr[0].axis('off'), axarr[1].axis('off')\n","  plt.show()"],"execution_count":0,"outputs":[]}]}